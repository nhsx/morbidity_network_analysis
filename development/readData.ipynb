{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0840c787",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import pprint\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.cm as cm\n",
    "from pyvis.network import Network\n",
    "from itertools import combinations\n",
    "from scipy.sparse import csr_matrix\n",
    "from matplotlib.colors import rgb2hex\n",
    "import community as community_louvain\n",
    "from matplotlib.colors import Normalize\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from statsmodels.stats.multitest import fdrcorrection\n",
    "from statsmodels.stats.contingency_tables import StratifiedTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5a4635d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class loadConfig():\n",
    "    \"\"\" Custom class to read, validate and \n",
    "        set defaults of YAML configuration file \n",
    "        \"\"\"\n",
    "    \n",
    "    def __init__(self, pathToYaml):\n",
    "        self.error = False\n",
    "        # Reserved string for mandatory arguments\n",
    "        self.mandatory = 'mandatory'\n",
    "        self.pathToYaml = pathToYaml\n",
    "        self.config = self._readYAML()\n",
    "        self._setDefault(self.config, self.default)\n",
    "        self._postProcessConfig()\n",
    "        if self.error:\n",
    "            logging.error(f'Expected format:\\n\\n{pprint.pformat(self.default)}')\n",
    "            raise ValueError('Invalid configuration.')\n",
    "\n",
    "    def _setDefault(self, config, default, path=''):\n",
    "        \"\"\" Recursively set default values. \"\"\"\n",
    "        for k in default:\n",
    "            if isinstance(default[k], dict):\n",
    "                if not isinstance(config[k], dict):\n",
    "                    logging.error(\n",
    "                        f'\"{config[k]}\" should be a dictionary.')\n",
    "                    self.error = True\n",
    "                else:\n",
    "                    self._setDefault(config.setdefault(k, {}), default[k], path=path+k)\n",
    "            else:\n",
    "                if (((k not in config) or (config[k] is None))\n",
    "                        and (default[k] == self.mandatory)):\n",
    "                    msg = f'{path}: {k}' if path else k\n",
    "                    logging.error(\n",
    "                        f'Missing mandatory config \"{msg}\".')\n",
    "                    self.error = True                  \n",
    "                config.setdefault(k, default[k])\n",
    "    \n",
    "    def _readYAML(self):\n",
    "        \"\"\" Custom validation \"\"\"\n",
    "        with open(self.pathToYaml, 'r') as stream:\n",
    "            return yaml.safe_load(stream)\n",
    "       \n",
    "    @property\n",
    "    def default(self):\n",
    "        \"\"\" Default values of configuration file. \"\"\"\n",
    "        return ({\n",
    "            'file': self.mandatory,\n",
    "            'codes': self.mandatory,\n",
    "            'strata': None,\n",
    "            'seperator': None,\n",
    "            'chunksize': None,\n",
    "        })\n",
    "    \n",
    "    \n",
    "    def _postProcessConfig(self):\n",
    "        \"\"\" Additional config modifications \"\"\"\n",
    "        config = self.config\n",
    "        config['allCols'] = []\n",
    "        if config['strata'] is not None:\n",
    "            config['allCols'] += config['strata']\n",
    "        if isinstance(config['codes'], list):\n",
    "            config['directed'] = False\n",
    "            config['codeCols'] = config['codes']\n",
    "            config['timeCols'] = None\n",
    "        else:\n",
    "            config['directed'] = True\n",
    "            config['codeCols'] = list(config['codes'].keys())\n",
    "            config['timeCols'] = list(config['codes'].values())\n",
    "            config['allCols'] += config['timeCols']\n",
    "        config['allCols'] += config['codeCols']\n",
    "        if config['chunksize'] is not None:\n",
    "            if not isinstance(config['chunksize'], int) or (config['chunksize'] <= 0):\n",
    "                logging.error(\n",
    "                    f'Invalid chunksize {config[\"chunksize\"]}\\n')\n",
    "                self.error = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c89356c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ea81fbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadConfigOld(config: str) -> dict:\n",
    "    \"\"\" Load and validate configuration file \"\"\"\n",
    "    with open(config, 'r') as stream:\n",
    "        config = yaml.safe_load(stream)\n",
    "    assert 'file' in config\n",
    "    assert 'codes' in config\n",
    "    config['allCols'] = []\n",
    "    if 'strata' in config:\n",
    "        config['allCols'] += config['strata']\n",
    "    else:\n",
    "        config['strata'] = None\n",
    "    if 'seperator' not in config:\n",
    "        config['seperator'] = None\n",
    "    if isinstance(config['codes'], list):\n",
    "        config['directed'] = False\n",
    "        config['codeCols'] = config['codes']\n",
    "        config['timeCols'] = None\n",
    "    else:\n",
    "        config['directed'] = True\n",
    "        config['codeCols'] = list(config['codes'].keys())\n",
    "        config['timeCols'] = list(config['codes'].values())\n",
    "        config['allCols'] += config['timeCols']\n",
    "    if 'chunksize' not in config:\n",
    "        config['chunksize'] = None\n",
    "    elif not isinstance(config['chunksize'], int) or (config['chunksize'] <= 0):\n",
    "        logging.error(\n",
    "            f'Invalid chunksize {config[\"chunksize\"]} - setting to None.\\n')\n",
    "        config['chunksize'] = None\n",
    "    config['allCols'] += config['codeCols']\n",
    "    return config\n",
    "\n",
    "\n",
    "def validateCols(df, config):\n",
    "    \"\"\" Check for missing columns in df \"\"\"\n",
    "    missingCols = set(config['allCols']) - set(df.columns)\n",
    "    if missingCols:\n",
    "        logging.error(f'{missingCols} not present in {config[\"file\"]}\\n')\n",
    "        raise ValueError\n",
    "    timeTypes = df[config['timeCols']].select_dtypes(\n",
    "        include=[np.number, np.datetime64])\n",
    "    invalidType = set(config['timeCols']) - set(timeTypes.columns)\n",
    "    if invalidType:\n",
    "        logging.error(\n",
    "            f'Invalid time type at columns {invalidType} in {config[\"file\"]}\\n')\n",
    "        raise ValueError\n",
    "\n",
    "        \n",
    "def checkDuplicates(df, config):\n",
    "    \"\"\" Check for duplicate codes in row \"\"\"\n",
    "    duplicates = df[config['codeCols']].apply(\n",
    "        lambda x: len(set(x.dropna())) < len(x.dropna()), axis=1)\n",
    "    \n",
    "    return duplicates[duplicates].index\n",
    "    \n",
    "\n",
    "def extractCodeTimes(x, codeCols, timeCols=None):\n",
    "    # Retrive unique codes and their associated time column\n",
    "    codeUniq = list(np.unique(x[codeCols].dropna(), return_index=True))\n",
    "    if len(codeUniq[0]) == 0:\n",
    "        return pd.Series([(), ()])\n",
    "    if timeCols is None:\n",
    "        timeUniq = tuple([True for i in range(len(codeUniq[0]))])\n",
    "    else:\n",
    "        timeUniq = [timeCols[i] for i in codeUniq[1]]\n",
    "        timeUniq = tuple(x[timeUniq].fillna(-1))\n",
    "    # Float node names not allowed by pyvis\n",
    "    if isinstance(codeUniq[0][0], float):\n",
    "        codes = tuple(int(c) for c in codeUniq[0])\n",
    "    else:\n",
    "        codes = tuple(codeUniq[0])\n",
    "    return pd.Series([codes, timeUniq])\n",
    "    \n",
    "\n",
    "def prepareData(df: pd.DataFrame, config: dict) -> pd.DataFrame:\n",
    "    \"\"\"Process ICD-10 multi-morbidity data.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame) : ICD-10 Data.\n",
    "        config (str) : Preloaded config file.\n",
    "\n",
    "    Returns:\n",
    "        Processed DataFrame of strata and ICD-10 codes.\n",
    "    \"\"\"\n",
    "    args = (config['codeCols'], config['timeCols'])\n",
    "    df = df.astype({col: object for col in config['codeCols']})\n",
    "    df[['codes', 'time']] = df.apply(extractCodeTimes, args=args, axis=1)\n",
    "    if config['strata']:\n",
    "        df['strata'] = df[config['strata']].apply(tuple, axis=1)\n",
    "    else:\n",
    "        df['strata'] = True\n",
    "    df = df.loc[:, ['strata', 'codes', 'time']]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def loadData(config: dict) -> pd.DataFrame:\n",
    "    \"\"\" Main function for loading data \"\"\"\n",
    "    #config = loadConfig(config)\n",
    "    data = pd.read_csv(\n",
    "        config['file'], sep=config['seperator'], \n",
    "        chunksize=config['chunksize'], iterator=True\n",
    "    )\n",
    "    allData = []\n",
    "    rowsWithDups = []\n",
    "    for i, chunk in enumerate(data):\n",
    "        if i == 0:\n",
    "            # Ensure all column names are in df\n",
    "            validateCols(chunk, config)\n",
    "        # Check for duplicate names \n",
    "        checkDuplicates(chunk, config)\n",
    "        allData.append(prepareData(chunk, config))\n",
    "    allData = pd.concat(allData)\n",
    "    allData.attrs['directed'] = config['directed']\n",
    "    return allData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0296b66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFullIndex(df: pd.DataFrame) -> pd.Series:\n",
    "    return df.reset_index()[['index', 'strata']].apply(tuple, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b6906505",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMMFrequency(df: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"Process ICD-10 multi-morbidity data.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): ICD-10 data processed by CMA.processData().\n",
    "\n",
    "    Returns:\n",
    "         Mulimorbidity frequency of pairwise ICD-10 codes.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        df['codes'].apply(\n",
    "            lambda x: [tuple(sorted(x)) for x in combinations(x, 2)])\n",
    "        .explode().dropna().value_counts().sort_values(ascending=False)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "90a386cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getICDlong(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Generate long-format ICD data, 1 code per row.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): ICD-10 data processed by CMA.processData().\n",
    "\n",
    "    Returns:\n",
    "         Long format ICD-10 codes.\n",
    "    \"\"\"\n",
    "    df_long = (\n",
    "        df.reset_index()\n",
    "        .rename({'index': 'ID'}, axis=1)\n",
    "        .set_index(['ID', 'strata'])\n",
    "        .apply(pd.Series.explode)\n",
    "        .reset_index()\n",
    "        .dropna()\n",
    "    )\n",
    "    if df.attrs['directed']:\n",
    "        df_long['time'] = df_long['time'].astype(int)\n",
    "    else:\n",
    "        df_long['time'] = df_long['time'].astype(bool)\n",
    "    return df_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fa4b8ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df2Sparse(df: pd.DataFrame, fullIndex: pd.Series) -> pd.DataFrame:\n",
    "    indexGrp = df.groupby(['ID', 'strata']).grouper\n",
    "    indexIdx = indexGrp.group_info[0]\n",
    "    colGroup = df.groupby(['codes']).grouper\n",
    "    colIdx = colGroup.group_info[0]\n",
    "    df_sparse = csr_matrix(\n",
    "        (df['time'].values, (indexIdx, colIdx)),\n",
    "        shape=(indexGrp.ngroups, colGroup.ngroups)\n",
    "    )\n",
    "    df_sparse = (\n",
    "        pd.DataFrame.sparse.from_spmatrix(\n",
    "            df_sparse, index=list(indexGrp), \n",
    "            columns=list(colGroup))\n",
    "        .reindex(fullIndex)\n",
    "        .fillna(0)\n",
    "    )\n",
    "    # Move stata to column\n",
    "    df_sparse['strata'] = df_sparse.index.map(lambda x: x[1]).values\n",
    "    # Set index as ID\n",
    "    df_sparse.index = df_sparse.index.map(lambda x: x[0]).values\n",
    "    return df_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "81f9f44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeStratifiedTable(\n",
    "    a1: np.array, \n",
    "    a2: np.array, \n",
    "    strataIndices: np.array,\n",
    "    exclude: np.array = None\n",
    ") -> np.array:\n",
    "    \"\"\" Generate set of stratified contigency tables \"\"\"\n",
    "    ctTables = []\n",
    "    exclude = None # REMOVE THIS\n",
    "    if exclude is None:\n",
    "        exclude = np.zeros(len(a1)).astype(bool)\n",
    "    a1 = a1[~exclude].copy()\n",
    "    a2 = a2[~exclude].copy()\n",
    "    for s in strataIndices:\n",
    "        ct = np.bincount(\n",
    "            2 * a1[s[~exclude]].astype(bool) \n",
    "            + a2[s[~exclude]].astype(bool), \n",
    "            minlength=4).reshape(2,2)\n",
    "        ctTables.append(ct)\n",
    "    return np.array(ctTables).swapaxes(0, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "76cf234c",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = '../example/config.yaml'\n",
    "config = loadConfig(config).config\n",
    "df = loadData(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c09b2ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fullIndex = getFullIndex(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f0cb4d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "codePairs = getMMFrequency(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "746400fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_long = getICDlong(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9e546032",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sp = df2Sparse(df_long, fullIndex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ca20947e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processLinks(links, stat='OR', minVal=1, alpha=0.01, minObs=1):\n",
    "    assert stat in links.columns\n",
    "    allNodes = set(links['ICDpair'].apply(pd.Series).melt()['value'].tolist())\n",
    "    sigLinks = links.loc[\n",
    "        (links['FDR'] < alpha)\n",
    "        & (links[stat] > minVal)\n",
    "        & (links['minObs'] >= minObs)\n",
    "    ]\n",
    "    allEdges = sigLinks.apply(lambda x: (*x['ICDpair'], x['OR']), axis=1).tolist()\n",
    "    return allNodes, allEdges\n",
    "\n",
    "\n",
    "def getGraphCentality(G, alphaMin=0.5):\n",
    "    assert 0 <= alphaMin < 1\n",
    "    centrality = nx.betweenness_centrality(G, weight='weight')\n",
    "    centrality = pd.Series(centrality).to_frame().rename({0: 'centrality'}, axis=1)\n",
    "    centrality['alpha'] = minmax_scale(centrality['centrality'], (alphaMin, 1))\n",
    "    return centrality\n",
    "\n",
    "\n",
    "def getGraphDegree(G, size=50, scale=10):\n",
    "    assert (size > 0) and (scale > 1)\n",
    "    degree = pd.DataFrame(G.degree()).set_index(0).rename({1: 'degree'}, axis=1)\n",
    "    degree['size'] = minmax_scale(degree['degree'], (size, size * scale))\n",
    "    return degree\n",
    "\n",
    "\n",
    "def getNodePartion(G, colours=None):\n",
    "    if colours is None:\n",
    "        colours = ([\n",
    "            (34,136,51), (204,187,68), (238,102,119),\n",
    "            (170,51,119), (68,119,170), (102,204,238),\n",
    "            (187,187,187)\n",
    "        ])\n",
    "    otherColour = colours[-1]\n",
    "    partitionColours = colours[:-1]\n",
    "    allPartitions = community_louvain.best_partition(G)\n",
    "    # Get largest partitions in network\n",
    "    mainPartions = (\n",
    "        pd.Series(allPartitions.values())\n",
    "        .value_counts()\n",
    "        .head(len(partitionColours)).index\n",
    "    )\n",
    "    mainPartions = dict(zip(mainPartions, partitionColours))\n",
    "    partitionInfo = {}\n",
    "    for node, partition in allPartitions.items():\n",
    "        if partition not in mainPartions:\n",
    "            partitionInfo[node] = (otherColour, partition)\n",
    "        else:\n",
    "            partitionInfo[node] = (mainPartions[partition], partition)\n",
    "    partitionInfo = (\n",
    "        pd.DataFrame(partitionInfo).T\n",
    "        .rename({0: 'partitionRGB', 1: 'partition'}, axis=1)\n",
    "    )\n",
    "    # Convert RGB to [0, 1] scale\n",
    "    partitionInfo['partitionRGB'] = partitionInfo['partitionRGB'].apply(\n",
    "        lambda x: (x[0] / 255, x[1] / 255, x[2] / 255)\n",
    "    )\n",
    "    return partitionInfo\n",
    "\n",
    "\n",
    "def getNodeRGB(G, cmap=cm.viridis):\n",
    "    norm = Normalize(vmin=0, vmax=(len(G.nodes()) - 1))\n",
    "    nodeRGB = {}\n",
    "    for i, node in enumerate(sorted(G.nodes())):\n",
    "        nodeRGB[node] = cmap(norm(i))[:3]\n",
    "    nodeRGB = pd.Series(nodeRGB).to_frame().rename({0: 'nodeRGB'}, axis=1)\n",
    "    return nodeRGB\n",
    "\n",
    "\n",
    "def getNodeSummary(G, alphaMin=0.5, size=50, scale=10, cmap=cm.viridis):\n",
    "    centrality = getGraphCentality(G, alphaMin)\n",
    "    degree = getGraphDegree(G, size, scale)\n",
    "    partitionRGB = getNodePartion(G)\n",
    "    nodeRGB = getNodeRGB(G, cmap)\n",
    "    summary = pd.merge(centrality, degree, left_index=True, right_index=True)\n",
    "    summary = pd.merge(summary, partitionRGB, left_index=True, right_index=True)\n",
    "    \n",
    "    summary = pd.merge(summary, nodeRGB, left_index=True, right_index=True)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "16eb8503",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = []\n",
    "for stratum in df_sp['strata'].unique():\n",
    "    indices.append(np.array(df_sp['strata'] == stratum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5e340b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "allLinks = []\n",
    "for i, ((m1, m2), count) in enumerate(codePairs.iteritems()):\n",
    "    a1 = np.array(df_sp[m1])\n",
    "    a2 = np.array(df_sp[m2])\n",
    "    # Exclude amiguous / missing time stamps\n",
    "    excludeAll = ((a1 == -1) & (a2 != 0)) | ((a2 == -1) & (a1 != 0))\n",
    "    # Direction specific exclusion\n",
    "    bothNonZero = (a1 != 0) & (a2 != 0)\n",
    "    exclude = (a1 >= a2) & bothNonZero\n",
    "    \n",
    "    tables = makeStratifiedTable(\n",
    "        a1, a2, indices, (exclude | excludeAll))\n",
    "    minObs = np.sum(tables, axis=2).min()\n",
    "    k = StratifiedTable(tables)\n",
    "    allLinks.append([\n",
    "        (m1, m2), count, minObs, k.oddsratio_pooled, k.riskratio_pooled,\n",
    "        k.test_equal_odds().pvalue, k.test_null_odds().pvalue\n",
    "    ])\n",
    "allLinks = pd.DataFrame(allLinks)\n",
    "allLinks.columns = ['ICDpair', 'count', 'minObs', 'OR', 'RR', 'pEqual', 'pNull']\n",
    "allLinks.loc[allLinks['pNull'].notna(), 'FDR'] = (\n",
    "    fdrcorrection(allLinks.loc[allLinks['pNull'].notna(), 'pNull'])[1]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d19f2331",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.01\n",
    "allNodes, allEdges = processLinks(allLinks, stat='RR', minVal=1, alpha=alpha, minObs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9187e58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.Graph()\n",
    "G.add_nodes_from(allNodes)\n",
    "G.add_weighted_edges_from(allEdges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ec114e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodeSummary = getNodeSummary(G, alphaMin=0.5, size=50, scale=10, cmap=cm.viridis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "412eb725",
   "metadata": {},
   "outputs": [],
   "source": [
    "largestCC = max(nx.connected_components(G), key=len)\n",
    "S = G.subgraph(largestCC).copy()\n",
    "S = G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ff9407e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "colourBy = 'node'\n",
    "for node in S.nodes():\n",
    "    S.nodes[node]['size'] = nodeSummary.loc[node, 'size']\n",
    "    S.nodes[node]['label'] = str(node)\n",
    "    S.nodes[node]['font'] = {'size': 200}\n",
    "    alpha = nodeSummary.loc[node, 'alpha']\n",
    "    rgb = nodeSummary.loc[node, f'{colourBy}RGB']\n",
    "    S.nodes[node]['color'] = rgb2hex((*rgb, alpha), keep_alpha=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "91839589",
   "metadata": {},
   "outputs": [],
   "source": [
    "allEdges = {edge: S.edges[edge]['weight'] for edge in S.edges()}\n",
    "allEdges = pd.Series(allEdges).to_frame().rename({0: 'OR'}, axis=1)\n",
    "allEdges['logOR'] = np.log(allEdges['OR'])\n",
    "allEdges['scaled'] = minmax_scale(allEdges['logOR'], (0.1, 1))\n",
    "# Truncate to 1 in case of rounding error\n",
    "allEdges['scaled'] = allEdges['scaled'].apply(lambda x: x if x < 1 else 1)\n",
    "allEdges = allEdges['scaled'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "bac2d0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for edge in S.edges():\n",
    "    weight = S.edges[edge]['weight']\n",
    "    S.edges[edge]['width'] = np.log(S.edges[edge]['weight'])\n",
    "    S.edges[edge]['color'] = rgb2hex((0, 0, 0, allEdges[edge]), keep_alpha=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "66bca99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "minDegree = 0\n",
    "remove = [x for x in S.nodes() if S.degree(x) < minDegree]\n",
    "S.remove_nodes_from(remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "98e137f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Network(height='75%', width='75%')\n",
    "net.from_nx(S)\n",
    "net.toggle_physics(True)\n",
    "net.barnes_hut()\n",
    "net.show('../example/exampleNet.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16478623",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
